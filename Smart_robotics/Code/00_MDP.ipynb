{"cells":[{"cell_type":"markdown","id":"159addea","metadata":{"id":"159addea"},"source":["# Markov Decision Processes"]},{"cell_type":"code","execution_count":null,"id":"e0bd31b1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e0bd31b1","executionInfo":{"status":"ok","timestamp":1706025317321,"user_tz":-60,"elapsed":23,"user":{"displayName":"Iker Etxebeste Arroyo","userId":"06101170301682302069"}},"outputId":"420977c0-cc4f-43f7-f087-ff0e4706fd08"},"outputs":[{"output_type":"stream","name":"stdout","text":["Python version: 3.10.12\n"]}],"source":["# Import libraries\n","import numpy as np\n","import platform\n","import math\n","from enum import Enum\n","from typing import Tuple, Generator, List\n","\n","# Check Python version\n","print(f'Python version: {platform.python_version()}')"]},{"cell_type":"markdown","id":"de8bb41f","metadata":{"id":"de8bb41f"},"source":["# Create the test scenario"]},{"cell_type":"markdown","id":"4b1dfc63","metadata":{"id":"4b1dfc63"},"source":["Let's create a simple map. We will define the map as a matrix, with each cell representing a state. The value of each cell indicates its category:\n","*  0: represents free space\n","*  1: represents the final goal\n","* -1: represents a hazard to avoid\n","* nan: represents a wall"]},{"cell_type":"code","execution_count":null,"id":"6786113d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6786113d","executionInfo":{"status":"ok","timestamp":1706025341038,"user_tz":-60,"elapsed":685,"user":{"displayName":"Iker Etxebeste Arroyo","userId":"06101170301682302069"}},"outputId":"ea36ddf0-f010-4a44-f4d1-11a597db0407"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 0.  0.  0.  1.]\n"," [ 0. nan  0. -1.]\n"," [ 0.  0.  0.  0.]]\n"]}],"source":["world = np.array([[0,0,0,1],[0, float('nan'), 0, -1],[0, 0, 0, 0]])\n","print(world)"]},{"cell_type":"markdown","id":"97301b16","metadata":{"id":"97301b16"},"source":["Let's define some helper functions.\n","\n","*is_valid* checks if a node with (x,y) coodinates is valid (i.e. within bounds) and it is not a wall"]},{"cell_type":"code","execution_count":null,"id":"01951aa3","metadata":{"id":"01951aa3"},"outputs":[],"source":["def is_valid(node:Tuple[int, int], world:np.ndarray):\n","    \"\"\"*is_valid* checks if a node with (x,y) coodinates is valid (i.e. within bounds) and it is not a wall\"\"\"\n","    (x,y) = node\n","    (w,h) = world.shape\n","    return x >= 0 and x < w and y >= 0 and y < h and not math.isnan(world[node]) # and world[node] >= 0"]},{"cell_type":"code","execution_count":null,"id":"0af7ab32","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0af7ab32","executionInfo":{"status":"ok","timestamp":1706025347584,"user_tz":-60,"elapsed":10,"user":{"displayName":"Iker Etxebeste Arroyo","userId":"06101170301682302069"}},"outputId":"854f359d-c2ca-42d2-dbc3-69e67bb69ad6"},"outputs":[{"output_type":"stream","name":"stdout","text":["False\n","True\n","False\n"]}],"source":["print(is_valid((-1, -1), world)) # False, outside of world\n","\n","print(is_valid((2, 1), world)) # OK\n","\n","print(is_valid((1, 1), world)) # False, is a wall"]},{"cell_type":"markdown","id":"bcc2d47f","metadata":{"id":"bcc2d47f"},"source":["We need a way to visit the neighbors of a cell:"]},{"cell_type":"code","execution_count":null,"id":"8a7e8094","metadata":{"id":"8a7e8094"},"outputs":[],"source":["def neighbor_iter(node: Tuple[int, int], world:np.ndarray)->Generator[Tuple[int, int], None, None]:\n","        \"\"\"List of valid nodes neighouring node\"\"\"\n","        (x,y) = node\n","        n = (x-1, y)\n","        if is_valid(n, world):\n","            yield n\n","        n = (x+1, y)\n","        if is_valid(n, world):\n","            yield n\n","        n = (x, y-1)\n","        if is_valid(n, world):\n","            yield n\n","        n = (x, y+1)\n","        if is_valid(n, world):\n","            yield n"]},{"cell_type":"code","execution_count":null,"id":"a0c512e5","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a0c512e5","executionInfo":{"status":"ok","timestamp":1706025363062,"user_tz":-60,"elapsed":549,"user":{"displayName":"Iker Etxebeste Arroyo","userId":"06101170301682302069"}},"outputId":"2142807b-92e3-424d-fd4f-80df6f9fb20a"},"outputs":[{"output_type":"stream","name":"stdout","text":["(1, 0)\n","(0, 1)\n"]}],"source":["for neighbor_cell in neighbor_iter((0,0), world):\n","    print(neighbor_cell)"]},{"cell_type":"markdown","metadata":{"id":"m3JhALOp9xjv"},"source":["Exercise: Find the neighbors of the wall cell"],"id":"m3JhALOp9xjv"},{"cell_type":"markdown","id":"558aa4c2","metadata":{"id":"558aa4c2"},"source":["And to draw the map"]},{"cell_type":"code","execution_count":null,"id":"74aa838a","metadata":{"id":"74aa838a"},"outputs":[],"source":["def draw_world(world: np.ndarray, path: List[Tuple[int, int]])->None:\n","    \"\"\"Draw the map and a path in it. The path appears as a list of numbers in the order the path is taken.\"\"\"\n","    w = world.copy()\n","    step = 1\n","    for cell in path:\n","        w[cell]=step\n","        step = step+1\n","    print(\"\\n\")\n","    print(w)"]},{"cell_type":"code","execution_count":null,"id":"7b1889f1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7b1889f1","executionInfo":{"status":"ok","timestamp":1706025377344,"user_tz":-60,"elapsed":580,"user":{"displayName":"Iker Etxebeste Arroyo","userId":"06101170301682302069"}},"outputId":"56541d92-4b6f-4d5b-ce2e-1629a4761027"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","[[ 0.  0.  0.  1.]\n"," [ 0. nan  0. -1.]\n"," [ 0.  0.  0.  0.]]\n"]}],"source":["draw_world(world, [])"]},{"cell_type":"code","execution_count":null,"id":"4576b5bb","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4576b5bb","executionInfo":{"status":"ok","timestamp":1706025383132,"user_tz":-60,"elapsed":605,"user":{"displayName":"Iker Etxebeste Arroyo","userId":"06101170301682302069"}},"outputId":"f3b9e083-62f4-4966-ec92-f89346fca10f"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","[[ 1.  2.  0.  1.]\n"," [ 0. nan  0. -1.]\n"," [ 0.  0.  0.  0.]]\n"]}],"source":["draw_world(world, [(0,0), (0,1)])"]},{"cell_type":"markdown","id":"394de466","metadata":{"id":"394de466"},"source":["## Markov Decision Processes (MDP)"]},{"cell_type":"markdown","id":"c88a3f62","metadata":{"id":"c88a3f62"},"source":["A Markov decision process takes into account that a robot can fail to complete a task. When planning a motion this means that the robot can end in a hazard area (i.e. stairs), even if the plan was perfect, just because it fails to complete a motion and ended in the wrong area.\n","\n","We can model this situation by defining a policy $\\pi$, a plan the guides the action of the robot in different circunstances. In this case the policy will be guided by the concept of utility (value):\n","* arriving at the destination has a certain utility,\n","* getting into a hazard has a negative utility,\n","* moving from cell to cell has a negative utility (or the more \"util\" plan would be to move in loops...).\n","\n","We can model these values as:"]},{"cell_type":"code","execution_count":null,"id":"77740f72","metadata":{"id":"77740f72"},"outputs":[],"source":["# This constants will define our transtition model T\n","TM_FORWARD = 0.8      # Probability of the robot moving along the planned direction\n","TM_LEFT = 0.1         # Probability of moving to the left\n","TM_RIGHT = 0.1        # Probability of moving to the right\n","\n","STEP_REWARD = -0.001  # Utility loss of moving\n","\n","# Discount\n","GAMMA = 1.0           # Loss of utility of previous steps: GAMMA = 1 no utility is loss, GAMMA = 0 previous steps don't count"]},{"cell_type":"markdown","id":"29d7ed15","metadata":{"id":"29d7ed15"},"source":["A key problem in the definition of a Markov process are transitions. The transition_test function which of the actions (move to left, right, up, down) is the most useful. It adds the utility of each action by summing the probability of moving to a neighbouring cell with the utility of this cell. The function will return the larger of this values:"]},{"cell_type":"code","execution_count":null,"id":"6167380d","metadata":{"id":"6167380d"},"outputs":[],"source":["def transition_test(node: Tuple[int, int], world: np.ndarray, value: np.ndarray) -> float:\n","    \"\"\"This function will test all posible actions considering the transition model and their feasibility\"\"\"\n","\n","    lst = []        # This list will store the value of each action\n","    (x,y) = node    # Let's split the coordinates of the state for easier usage\n","\n","    # Move to left\n","\n","    sum = 0\n","    forward_node = (x, y-1)\n","    left_node    = (x+1, y)\n","    right_node   = (x-1, y)\n","    if is_valid(forward_node, world):\n","        sum +=  TM_FORWARD * value[forward_node]\n","    if is_valid(left_node, world):\n","        sum += TM_LEFT * value[left_node]\n","    if is_valid(right_node, world):\n","        sum += TM_RIGHT * value[right_node]\n","\n","    lst.append(sum)\n","\n","    # Move Up\n","\n","    sum = 0\n","    forward_node = (x+1, y)\n","    left_node    = (x, y-1)\n","    right_node   = (x, y+1)\n","    if is_valid(forward_node, world):\n","        sum += TM_FORWARD * value[forward_node]\n","    if is_valid(left_node, world):\n","        sum += TM_LEFT * value[left_node]\n","    if is_valid(right_node, world):\n","        sum += TM_RIGHT * value[right_node]\n","\n","    lst.append(sum)\n","\n","    # Move Right\n","\n","    sum = 0\n","    forward_node = (x, y+1)\n","    left_node    = (x-1, y)\n","    right_node   = (x+1, y)\n","    if is_valid(forward_node, world):\n","        sum += TM_FORWARD * value[forward_node]\n","    if is_valid(left_node, world):\n","        sum += TM_LEFT * value[left_node]\n","    if is_valid(right_node, world):\n","        sum += TM_RIGHT * value[right_node]\n","\n","    lst.append(sum)\n","\n","    # Move Down\n","\n","    sum = 0\n","    forward_node = (x-1, y)\n","    left_node    = (x, y+1)\n","    right_node   = (x, y-1)\n","    if is_valid(forward_node, world):\n","        sum += TM_FORWARD * value[forward_node]\n","    if is_valid(left_node, world):\n","        sum += TM_LEFT * value[left_node]\n","    if is_valid(right_node, world):\n","        sum += TM_RIGHT * value[right_node]\n","\n","    lst.append(sum)\n","\n","    return max(lst)"]},{"cell_type":"markdown","id":"477f6f2e","metadata":{"id":"477f6f2e"},"source":["The markov function computes the utility of our map. It iterates the utility of each cell according to $u_{i,j}=r_{i,j}+\\Gamma*T_{i,j}$, where $r$ is the expected rewars, $\\Gamma$ is the decay value of the iteration and $T_{i,j}$ is the utility of the transition:"]},{"cell_type":"code","execution_count":null,"id":"add5be16","metadata":{"id":"add5be16"},"outputs":[],"source":["def markov(world:np.ndarray, rewards: np.ndarray)->np.ndarray:\n","    \"\"\"Compute the utility of the map. Rewards and utility are stores in maps with the same shape as the map\"\"\"\n","    value = np.zeros_like(world)\n","\n","    while True:\n","        up = np.copy(value) # Make a copy of the current utility for comparison\n","\n","        # For each cell of the map\n","        for x in range(0, world.shape[0]):\n","            for y in range(0, world.shape[1]):\n","                if not math.isnan(world[x,y]): # If it's not a wall\n","                    if world[x,y] == 1: # Don't change the utility of the goal...\n","                        value[x,y]=1\n","                    elif world[x,y] == -1: # ... or the hazards\n","                        value[x,y]=-1\n","                    else: # For all other cells, update the utility using Bellman's Equation: V(ğ‘ )=ğ‘…(ğ‘ )+maxâ¡ âˆ‘ğ‘ â€²[ğ‘‡(ğ‘ , ğ‘, ğ‘ ^â€² )ğ‘‰(ğ‘ ^â€²)]\n","                        value[x,y] = rewards[x,y]+GAMMA*transition_test((x,y), world, up)\n","                else: # if it's a wall then we fix its utility to a fixed low value\n","                    value[x,y] = -10000\n","\n","        #print(utility)\n","        #print(rewards)\n","\n","        if np.linalg.norm(value-up)<1e-7: # Stop when the change is small enough\n","            return value"]},{"cell_type":"markdown","id":"d4d65ad9","metadata":{"id":"d4d65ad9"},"source":["The trace function uses the utility matrix to generate a path. It starts searching for the motions with larger utility at each step (simulating a gradient ascent)"]},{"cell_type":"code","execution_count":null,"id":"8658f246","metadata":{"id":"8658f246"},"outputs":[],"source":["def trace(node:Tuple[int, int], world:np.ndarray, value:np.ndarray)->List[Tuple[int, int]]:\n","    \"\"\"Trace the path\"\"\"\n","    curr = node\n","    u = value[node]\n","    path = [node]\n","    next = ()\n","    while True:\n","        for node in neighbor_iter(curr, world):\n","            if u < value[node]:\n","                u = value[node]\n","                next = node\n","        if next == ():\n","            return path\n","        else:\n","            path.append(next)\n","            curr = next\n","            next = ()"]},{"cell_type":"markdown","id":"3bd8d6e9","metadata":{"id":"3bd8d6e9"},"source":["## Example 1"]},{"cell_type":"markdown","id":"3fc8cc93","metadata":{"id":"3fc8cc93"},"source":["We can also set-up a reward map for our problem"]},{"cell_type":"code","execution_count":null,"id":"d4b566e5","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d4b566e5","executionInfo":{"status":"ok","timestamp":1706025493911,"user_tz":-60,"elapsed":580,"user":{"displayName":"Iker Etxebeste Arroyo","userId":"06101170301682302069"}},"outputId":"e9fa785b-0127-4e54-feca-9e1fdc4c74c8"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[-0.001 -0.001 -0.001  1.   ]\n"," [-0.001    nan -0.001 -1.   ]\n"," [-0.001 -0.001 -0.001 -0.001]]\n"]}],"source":["rewards = np.array([[STEP_REWARD, STEP_REWARD,STEP_REWARD, 1],[STEP_REWARD, float('NaN'), STEP_REWARD, -1],[STEP_REWARD, STEP_REWARD, STEP_REWARD, STEP_REWARD]])\n","print(rewards)"]},{"cell_type":"markdown","id":"31fe5a65","metadata":{"id":"31fe5a65"},"source":["Finally we can run the code and generate a path:"]},{"cell_type":"code","execution_count":null,"id":"0f0b9698","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0f0b9698","executionInfo":{"status":"ok","timestamp":1706025504407,"user_tz":-60,"elapsed":627,"user":{"displayName":"Iker Etxebeste Arroyo","userId":"06101170301682302069"}},"outputId":"13191e26-0fa7-4eb5-9389-b0f613d89d4b"},"outputs":[{"output_type":"stream","name":"stdout","text":["[(2, 1), (2, 2), (1, 2), (0, 2), (0, 3)]\n","\n","\n","[[ 0.  0.  4.  5.]\n"," [ 0. nan  3. -1.]\n"," [ 0.  1.  2.  0.]]\n"]}],"source":["value = markov(world, rewards)\n","\n","path = trace((2,1), world, value)\n","print(path)\n","\n","draw_world(world, path)"]},{"cell_type":"code","execution_count":null,"id":"2b7cdc67","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2b7cdc67","executionInfo":{"status":"ok","timestamp":1706025538234,"user_tz":-60,"elapsed":552,"user":{"displayName":"Iker Etxebeste Arroyo","userId":"06101170301682302069"}},"outputId":"9f682d91-e675-4a1a-82c8-aa977768d6b6"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 5.94456522e-01  6.85000000e-01  8.57500000e-01  1.00000000e+00]\n"," [ 4.74565217e-01 -1.00000000e+04  5.85000000e-01 -1.00000000e+00]\n"," [ 4.22056930e-01  4.34047567e-01  5.43809513e-01  3.34047567e-01]]\n"]}],"source":["print(value)"]},{"cell_type":"markdown","id":"f1d30a7f","metadata":{"id":"f1d30a7f"},"source":["## Example 2\n","Let's generate a second map"]},{"cell_type":"code","execution_count":null,"id":"947f110a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"947f110a","executionInfo":{"status":"ok","timestamp":1706025539468,"user_tz":-60,"elapsed":9,"user":{"displayName":"Iker Etxebeste Arroyo","userId":"06101170301682302069"}},"outputId":"b35be8bf-5eff-4190-f1fe-a24a8eaf308d"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 0.  0.  0.  0.  1.]\n"," [nan  0. nan nan -1.]\n"," [ 0.  0.  0.  0.  0.]\n"," [ 0. nan nan  0. -1.]\n"," [ 0.  0.  0.  0.  0.]]\n"]}],"source":["world2 = np.array([[0,0,0,0,1],[float('nan'), 0, float('nan'), float('nan'), -1],[0, 0, 0, 0, 0], [0, float('nan'), float('nan'), 0, -1], [0, 0, 0, 0, 0]])\n","print(world2)"]},{"cell_type":"code","execution_count":null,"id":"13901674","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"13901674","executionInfo":{"status":"ok","timestamp":1706025549819,"user_tz":-60,"elapsed":10,"user":{"displayName":"Iker Etxebeste Arroyo","userId":"06101170301682302069"}},"outputId":"b71c5bf2-461f-4062-d3e9-f1782e8925fa"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[-0.001 -0.001 -0.001 -0.001  1.   ]\n"," [ 0.    -0.001  0.     0.     1.   ]\n"," [-0.001 -0.001 -0.001 -0.001 -0.001]\n"," [-0.001  0.     0.    -0.001  1.   ]\n"," [-0.001 -0.001 -0.001 -0.001 -0.001]]\n"]}],"source":["rewards2 = (world2 == 0).astype(float)*STEP_REWARD+(world2 == 1).astype(float)+(world2 == -1).astype(float)\n","print(rewards2)"]},{"cell_type":"code","execution_count":null,"id":"cdb723c6","metadata":{"id":"cdb723c6"},"outputs":[],"source":["value = markov(world2, rewards2)"]},{"cell_type":"code","execution_count":null,"id":"b09280f4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b09280f4","executionInfo":{"status":"ok","timestamp":1706025567231,"user_tz":-60,"elapsed":573,"user":{"displayName":"Iker Etxebeste Arroyo","userId":"06101170301682302069"}},"outputId":"59a25792-0036-4ca1-9710-e62d0a5abeed"},"outputs":[{"output_type":"stream","name":"stdout","text":["[(4, 0), (3, 0), (2, 0), (2, 1), (1, 1), (0, 1), (0, 2), (0, 3), (0, 4)]\n","\n","\n","[[ 0.  6.  7.  8.  9.]\n"," [nan  5. nan nan -1.]\n"," [ 3.  4.  0.  0.  0.]\n"," [ 2. nan nan  0. -1.]\n"," [ 1.  0.  0.  0.  0.]]\n"]}],"source":["path = trace((4,0), world2, value)\n","print(path)\n","\n","draw_world(world2, path)"]},{"cell_type":"markdown","id":"10d68e32","metadata":{"id":"10d68e32"},"source":["Let's trace a path from point (4,4)"]},{"cell_type":"code","execution_count":null,"id":"c11aef13","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c11aef13","executionInfo":{"status":"ok","timestamp":1706025609011,"user_tz":-60,"elapsed":588,"user":{"displayName":"Iker Etxebeste Arroyo","userId":"06101170301682302069"}},"outputId":"aa15e020-355a-44f7-afe1-b444ec483ecd"},"outputs":[{"output_type":"stream","name":"stdout","text":["[(4, 4), (4, 3), (4, 2), (4, 1), (4, 0), (3, 0), (2, 0), (2, 1), (1, 1), (0, 1), (0, 2), (0, 3), (0, 4)]\n","\n","\n","[[ 0. 10. 11. 12. 13.]\n"," [nan  9. nan nan -1.]\n"," [ 7.  8.  0.  0.  0.]\n"," [ 6. nan nan  0. -1.]\n"," [ 5.  4.  3.  2.  1.]]\n"]}],"source":["path = trace((4,4), world2, value)\n","print(path)\n","\n","draw_world(world2, path)"]},{"cell_type":"markdown","id":"ce81e31b","metadata":{"id":"ce81e31b"},"source":["## Example 3\n","\n","Now let's test the examples changing some parameters"]},{"cell_type":"code","execution_count":null,"id":"062c5696","metadata":{"id":"062c5696"},"outputs":[],"source":["TM_FORWARD = 0.98\n","TM_LEFT = 0.01\n","TM_RIGHT = 0.01\n","\n","STEP_REWARD = -0.01\n","GAMMA = 0.8"]},{"cell_type":"code","execution_count":null,"id":"8e2d1822","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8e2d1822","executionInfo":{"status":"ok","timestamp":1706025641401,"user_tz":-60,"elapsed":8,"user":{"displayName":"Iker Etxebeste Arroyo","userId":"06101170301682302069"}},"outputId":"88a41c9f-faee-4b07-c0e1-7a6b023fad55"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[-0.01 -0.01 -0.01  1.  ]\n"," [-0.01  0.   -0.01  1.  ]\n"," [-0.01 -0.01 -0.01 -0.01]]\n"]}],"source":["rewards = (world == 0).astype(float)*STEP_REWARD+(world == 1).astype(float)+(world == -1).astype(float)\n","print(rewards)"]},{"cell_type":"code","execution_count":null,"id":"19d02499","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"19d02499","executionInfo":{"status":"ok","timestamp":1706025687247,"user_tz":-60,"elapsed":537,"user":{"displayName":"Iker Etxebeste Arroyo","userId":"06101170301682302069"}},"outputId":"28f0df06-e698-4766-8891-b66a34c64612"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 4.63645356e-01  6.00532363e-01  7.78740259e-01  1.00000000e+00]\n"," [ 3.53497959e-01 -1.00000000e+04  5.92532363e-01 -1.00000000e+00]\n"," [ 2.69948100e-01  3.50712736e-01  4.60092776e-01  3.42712736e-01]]\n"]}],"source":["value = markov(world, rewards)\n","print(value)"]},{"cell_type":"code","execution_count":null,"id":"6bd5dd6b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6bd5dd6b","executionInfo":{"status":"ok","timestamp":1706025688471,"user_tz":-60,"elapsed":9,"user":{"displayName":"Iker Etxebeste Arroyo","userId":"06101170301682302069"}},"outputId":"96798faf-00db-48f0-9975-ab18420569ef"},"outputs":[{"output_type":"stream","name":"stdout","text":["[(2, 1), (2, 2), (1, 2), (0, 2), (0, 3)]\n","\n","\n","[[ 0.  0.  4.  5.]\n"," [ 0. nan  3. -1.]\n"," [ 0.  1.  2.  0.]]\n"]}],"source":["path = trace((2,1), world, value)\n","print(path)\n","\n","draw_world(world, path)"]},{"cell_type":"code","execution_count":null,"id":"9385c53f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9385c53f","executionInfo":{"status":"ok","timestamp":1706025692089,"user_tz":-60,"elapsed":516,"user":{"displayName":"Iker Etxebeste Arroyo","userId":"06101170301682302069"}},"outputId":"ba4b3b28-9c08-4e0e-985a-62f074745379"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[-0.01 -0.01 -0.01 -0.01  1.  ]\n"," [ 0.   -0.01  0.    0.   -1.  ]\n"," [-0.01 -0.01 -0.01 -0.01 -0.01]\n"," [-0.01  0.    0.   -0.01 -1.  ]\n"," [-0.01 -0.01 -0.01 -0.01 -0.01]]\n"]}],"source":["rewards2 = (world2 == 0).astype(float)*STEP_REWARD+(world2 == 1).astype(float)+(world2 == -1).astype(float)*(-1.0)\n","print(rewards2)\n","utility2 = np.zeros_like(world2)\n","\n","value = markov(world2, rewards2)"]},{"cell_type":"code","execution_count":null,"id":"8394f6e8","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8394f6e8","executionInfo":{"status":"ok","timestamp":1706025698150,"user_tz":-60,"elapsed":8,"user":{"displayName":"Iker Etxebeste Arroyo","userId":"06101170301682302069"}},"outputId":"81aae948-0896-40c3-df2c-a3fb2b41aa33"},"outputs":[{"output_type":"stream","name":"stdout","text":["[(4, 4), (4, 3), (3, 3), (2, 3), (2, 2), (2, 1), (1, 1), (0, 1), (0, 2), (0, 3), (0, 4)]\n","\n","\n","[[ 0.  8.  9. 10. 11.]\n"," [nan  7. nan nan -1.]\n"," [ 0.  6.  5.  4.  0.]\n"," [ 0. nan nan  3. -1.]\n"," [ 0.  0.  0.  2.  1.]]\n"]}],"source":["path = trace((4,4), world2, value)\n","print(path)\n","\n","draw_world(world2, path)"]},{"cell_type":"markdown","metadata":{"id":"l8lBWmIY9xkN"},"source":["# Exercise\n","\n","Create your own adventure. Create a map at least of size 5x5 and set several walls and hazards. Also define a cell goal and transition parameters as well. Test it and analyze the results.\n","\n","- Is it possible to set several goals?\n","- Could the goals have different values (utility)? What would happen?\n","\n","- What information do we need to provide to the algorithm?\n","- Is it realistical to have this information?\n","\n","- We have worked with a world where every state 's' and action 'a' have discrete values. Could we work with continuous states or actions?\n"],"id":"l8lBWmIY9xkN"},{"cell_type":"markdown","metadata":{"id":"hO_EE9DX9xkO"},"source":["## First map, with just one goal"],"id":"hO_EE9DX9xkO"},{"cell_type":"code","source":["map = np.array([[0,0,float('nan'),0,1],[float('nan'), 0, float('nan'), 0, -1],[0, 0, 0, 0, 0], [0, float('nan'), float('nan'), 0, -1], [0, 0, 0, 0, 0]])\n","print(map)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ssUKph1vDqeA","executionInfo":{"status":"ok","timestamp":1706026732336,"user_tz":-60,"elapsed":6,"user":{"displayName":"Iker Etxebeste Arroyo","userId":"06101170301682302069"}},"outputId":"222a173b-89d9-4736-d80e-6ed85977ceb5"},"id":"ssUKph1vDqeA","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 0.  0. nan  0.  1.]\n"," [nan  0. nan  0. -1.]\n"," [ 0.  0.  0.  0.  0.]\n"," [ 0. nan nan  0. -1.]\n"," [ 0.  0.  0.  0.  0.]]\n"]}]},{"cell_type":"markdown","source":["## Set Parameters"],"metadata":{"id":"tQYVBs8REpdV"},"id":"tQYVBs8REpdV"},{"cell_type":"code","source":["# This constants will define our transtition model T\n","TM_FORWARD = 0.8      # Probability of the robot moving along the planned direction\n","TM_LEFT = 0.1         # Probability of moving to the left\n","TM_RIGHT = 0.1   # Probability of moving to the right\n","\n","STEP_REWARD = -0.001  # Utility loss of moving\n","\n","# Discount\n","GAMMA = 1.0           # Loss of utility of previous steps: GAMMA = 1 no utility is loss, GAMMA = 0 previous steps don't count"],"metadata":{"id":"SnQtJh4mEbz9"},"id":"SnQtJh4mEbz9","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" ## Set rewards with parameters"],"metadata":{"id":"tFVB4bbBFizy"},"id":"tFVB4bbBFizy"},{"cell_type":"code","source":["rewardsmap = (map == 0).astype(float)*STEP_REWARD+(map == 1).astype(float)+(map == -1).astype(float)\n","print(rewardsmap)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9OF61cX2ExBm","executionInfo":{"status":"ok","timestamp":1706026799163,"user_tz":-60,"elapsed":581,"user":{"displayName":"Iker Etxebeste Arroyo","userId":"06101170301682302069"}},"outputId":"8273ac88-7fc3-45a0-d4e4-4215671cb4b7"},"id":"9OF61cX2ExBm","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[-0.001 -0.001  0.    -0.001  1.   ]\n"," [ 0.    -0.001  0.    -0.001  1.   ]\n"," [-0.001 -0.001 -0.001 -0.001 -0.001]\n"," [-0.001  0.     0.    -0.001  1.   ]\n"," [-0.001 -0.001 -0.001 -0.001 -0.001]]\n"]}]},{"cell_type":"code","source":["value = markov(map, rewardsmap)\n","print(value)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DXmxM3uGFE0g","executionInfo":{"status":"ok","timestamp":1706026818679,"user_tz":-60,"elapsed":6,"user":{"displayName":"Iker Etxebeste Arroyo","userId":"06101170301682302069"}},"outputId":"97c9dd59-88e8-45eb-d17b-4553ec4c5234"},"id":"DXmxM3uGFE0g","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 2.02123039e-01  2.53903814e-01 -1.00000000e+04  8.57500000e-01\n","   1.00000000e+00]\n"," [-1.00000000e+04  2.93364387e-01 -1.00000000e+04  5.85000000e-01\n","  -1.00000000e+00]\n"," [ 3.18765637e-01  3.67955486e-01  4.24523809e-01  5.31904762e-01\n","   2.24523809e-01]\n"," [ 2.54012510e-01 -1.00000000e+04 -1.00000000e+04  3.24523809e-01\n","  -1.00000000e+00]\n"," [ 2.20957157e-01  1.87471647e-01  2.35589559e-01  2.95736959e-01\n","   1.35589559e-01]]\n"]}]},{"cell_type":"code","source":["path = trace((4,0), map, value)\n","print(path)\n","\n","draw_world(map, path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Mwfde0DFh0y","executionInfo":{"status":"ok","timestamp":1706026821926,"user_tz":-60,"elapsed":6,"user":{"displayName":"Iker Etxebeste Arroyo","userId":"06101170301682302069"}},"outputId":"75da9e93-4708-40bf-8d9f-724a355f253d"},"id":"9Mwfde0DFh0y","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[(4, 0), (3, 0), (2, 0), (2, 1), (2, 2), (2, 3), (1, 3), (0, 3), (0, 4)]\n","\n","\n","[[ 0.  0. nan  8.  9.]\n"," [nan  0. nan  7. -1.]\n"," [ 3.  4.  5.  6.  0.]\n"," [ 2. nan nan  0. -1.]\n"," [ 1.  0.  0.  0.  0.]]\n"]}]},{"cell_type":"markdown","source":["## 2.Map with more than one goal"],"metadata":{"id":"tw7cZt3jIGBc"},"id":"tw7cZt3jIGBc"},{"cell_type":"code","source":["map2 = np.array([[0,0,float('nan'),0,1],[float('nan'), 0, float('nan'), 0, -1],[0, 0, 0, 0, 0], [0, float('nan'), float('nan'), 0, -1], [0, 0, 0, 0, 1]])\n","print(map2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JzyIO7J2IJmd","executionInfo":{"status":"ok","timestamp":1706027252328,"user_tz":-60,"elapsed":11,"user":{"displayName":"Iker Etxebeste Arroyo","userId":"06101170301682302069"}},"outputId":"a60e661a-ba29-4315-e0b5-d031217f09d7"},"id":"JzyIO7J2IJmd","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 0.  0. nan  0.  1.]\n"," [nan  0. nan  0. -1.]\n"," [ 0.  0.  0.  0.  0.]\n"," [ 0. nan nan  0. -1.]\n"," [ 0.  0.  0.  0.  1.]]\n"]}]},{"cell_type":"code","source":["rewardsmap2 = (map2 == 0).astype(float)*STEP_REWARD+(map2 == 1).astype(float)+(map2 == -1).astype(float)\n","print(rewardsmap2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-gdrWzZuIMet","executionInfo":{"status":"ok","timestamp":1706027255788,"user_tz":-60,"elapsed":577,"user":{"displayName":"Iker Etxebeste Arroyo","userId":"06101170301682302069"}},"outputId":"b8aac9b1-e034-4a85-fa55-803ddd6f9d98"},"id":"-gdrWzZuIMet","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[-0.001 -0.001  0.    -0.001  1.   ]\n"," [ 0.    -0.001  0.    -0.001  1.   ]\n"," [-0.001 -0.001 -0.001 -0.001 -0.001]\n"," [-0.001  0.     0.    -0.001  1.   ]\n"," [-0.001 -0.001 -0.001 -0.001  1.   ]]\n"]}]},{"cell_type":"code","source":["value2 = markov(map2, rewardsmap2)\n","print(value2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Eycn8wIJIOQ2","executionInfo":{"status":"ok","timestamp":1706027255789,"user_tz":-60,"elapsed":13,"user":{"displayName":"Iker Etxebeste Arroyo","userId":"06101170301682302069"}},"outputId":"21c9c227-85ee-4fa9-f04f-b425f60a2370"},"id":"Eycn8wIJIOQ2","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 2.02122965e-01  2.53903799e-01 -1.00000000e+04  8.57500000e-01\n","   1.00000000e+00]\n"," [-1.00000000e+04  2.93364378e-01 -1.00000000e+04  5.85000000e-01\n","  -1.00000000e+00]\n"," [ 3.38647721e-01  3.67955484e-01  4.24523808e-01  5.31904762e-01\n","   2.24523808e-01]\n"," [ 3.78565217e-01 -1.00000000e+04 -1.00000000e+04  5.85000000e-01\n","  -1.00000000e+00]\n"," [ 4.74456522e-01  5.47000000e-01  6.85000000e-01  8.57500000e-01\n","   1.00000000e+00]]\n"]}]},{"cell_type":"code","source":["path2 = trace((4,0), map2, value2)\n","print(path2)\n","\n","draw_world(map2, path2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r14bi-XxIQVV","executionInfo":{"status":"ok","timestamp":1706027256254,"user_tz":-60,"elapsed":9,"user":{"displayName":"Iker Etxebeste Arroyo","userId":"06101170301682302069"}},"outputId":"35c4bb65-b76d-4df8-bd7e-47ee7f39e9d2"},"id":"r14bi-XxIQVV","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[(4, 0), (4, 1), (4, 2), (4, 3), (4, 4)]\n","\n","\n","[[ 0.  0. nan  0.  1.]\n"," [nan  0. nan  0. -1.]\n"," [ 0.  0.  0.  0.  0.]\n"," [ 0. nan nan  0. -1.]\n"," [ 1.  2.  3.  4.  5.]]\n"]}]},{"cell_type":"code","source":["path2 = trace((0,0), map2, value2)\n","print(path2)\n","\n","draw_world(map2, path2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TsUrkwK_IjId","executionInfo":{"status":"ok","timestamp":1706027237831,"user_tz":-60,"elapsed":8,"user":{"displayName":"Iker Etxebeste Arroyo","userId":"06101170301682302069"}},"outputId":"dc026d8e-d2e0-4b14-ef49-545563c2b287"},"id":"TsUrkwK_IjId","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[(0, 0), (0, 1), (1, 1), (2, 1), (2, 2), (2, 3), (1, 3), (0, 3), (0, 4)]\n","\n","\n","[[ 1.  2. nan  8.  9.]\n"," [nan  3. nan  7. -1.]\n"," [ 0.  4.  5.  6.  0.]\n"," [ 0. nan nan  0. -1.]\n"," [ 0.  0.  0.  0.  1.]]\n"]}]},{"cell_type":"markdown","source":["## As we can see, it will always take the shortest path to the reward, depending on where it starts. Now we will try increasing the distant reward significantly to see which one it chooses."],"metadata":{"id":"moP5l_WsIslf"},"id":"moP5l_WsIslf"},{"cell_type":"markdown","source":["## 3. Setting higher reward"],"metadata":{"id":"qdxwHikuJQhE"},"id":"qdxwHikuJQhE"},{"cell_type":"code","source":["map3 = np.array([[0,0,float('nan'),0,1],[float('nan'), 0, float('nan'), 0, -1],[0, 0, 0, 0, 0], [0, float('nan'), float('nan'), 0, -1], [0, 0, 0, 0, 2]])\n","print(map3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vU2LohYfJWKM","executionInfo":{"status":"ok","timestamp":1706027832515,"user_tz":-60,"elapsed":3,"user":{"displayName":"Iker Etxebeste Arroyo","userId":"06101170301682302069"}},"outputId":"8c8e6c61-ab79-461b-e6ca-72af7b1efcbb"},"id":"vU2LohYfJWKM","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 0.  0. nan  0.  1.]\n"," [nan  0. nan  0. -1.]\n"," [ 0.  0.  0.  0.  0.]\n"," [ 0. nan nan  0. -1.]\n"," [ 0.  0.  0.  0.  2.]]\n"]}]},{"cell_type":"code","source":["rewardsmap3 = (map3 == 0).astype(float)*STEP_REWARD+(map3 == 1 ).astype(float)+(map3 == 2 ).astype(float)+(map3 == -1).astype(float)\n","print(rewardsmap3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5aAsQcU5Jh3W","executionInfo":{"status":"ok","timestamp":1706027833774,"user_tz":-60,"elapsed":467,"user":{"displayName":"Iker Etxebeste Arroyo","userId":"06101170301682302069"}},"outputId":"8b9c7bb9-de95-4738-fa89-6cd6677a281d"},"id":"5aAsQcU5Jh3W","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[-0.001 -0.001  0.    -0.001  1.   ]\n"," [ 0.    -0.001  0.    -0.001  1.   ]\n"," [-0.001 -0.001 -0.001 -0.001 -0.001]\n"," [-0.001  0.     0.    -0.001  1.   ]\n"," [-0.001 -0.001 -0.001 -0.001  1.   ]]\n"]}]},{"cell_type":"code","source":["value3 = markov(map3, rewardsmap3)\n","print(value3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H6izJAQYJs6j","executionInfo":{"status":"ok","timestamp":1706027834751,"user_tz":-60,"elapsed":1,"user":{"displayName":"Iker Etxebeste Arroyo","userId":"06101170301682302069"}},"outputId":"8a0d8330-7f23-4d77-b45a-8de98035f200"},"id":"H6izJAQYJs6j","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 6.95983719e-01  8.71229689e-01 -1.00000000e+04  1.17503256e+00\n","   1.00000000e+00]\n"," [-1.00000000e+04  1.00328915e+00 -1.00000000e+04  1.34504070e+00\n","  -1.00000000e+00]\n"," [ 1.11568777e+00  1.25536147e+00  1.44504070e+00  1.80755092e+00\n","   1.24504070e+00]\n"," [ 1.12398625e+00 -1.00000000e+04 -1.00000000e+04  1.92442847e+00\n","  -1.00000000e+00]\n"," [ 1.40623282e+00  1.61854278e+00  2.02442847e+00  2.53178562e+00\n","   2.92542847e+00]]\n"]}]},{"cell_type":"code","source":["path3 = trace((4,0), map3, value3)\n","print(path3)\n","\n","draw_world(map3, path3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dZX0yu_dJu5M","executionInfo":{"status":"ok","timestamp":1706027835985,"user_tz":-60,"elapsed":2,"user":{"displayName":"Iker Etxebeste Arroyo","userId":"06101170301682302069"}},"outputId":"7d2d9d71-48cb-4257-aba0-1a7626fcb03f"},"id":"dZX0yu_dJu5M","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[(4, 0), (4, 1), (4, 2), (4, 3), (4, 4)]\n","\n","\n","[[ 0.  0. nan  0.  1.]\n"," [nan  0. nan  0. -1.]\n"," [ 0.  0.  0.  0.  0.]\n"," [ 0. nan nan  0. -1.]\n"," [ 1.  2.  3.  4.  5.]]\n"]}]},{"cell_type":"code","source":["path3 = trace((0,0), map3, value3)\n","print(path3)\n","\n","draw_world(map3, path3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fP5v5YdhJwJk","executionInfo":{"status":"ok","timestamp":1706027837073,"user_tz":-60,"elapsed":2,"user":{"displayName":"Iker Etxebeste Arroyo","userId":"06101170301682302069"}},"outputId":"eb12789f-a34e-4f39-b037-604ef240f6f4"},"id":"fP5v5YdhJwJk","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[(0, 0), (0, 1), (1, 1), (2, 1), (2, 2), (2, 3), (3, 3), (4, 3), (4, 4)]\n","\n","\n","[[ 1.  2. nan  0.  1.]\n"," [nan  3. nan  0. -1.]\n"," [ 0.  4.  5.  6.  0.]\n"," [ 0. nan nan  7. -1.]\n"," [ 0.  0.  0.  8.  9.]]\n"]}]},{"cell_type":"markdown","source":["## As we can see, the second path has changed. By doubling the reward at the bottom, we can see that it now prefers the lower path, even though it previously chose the upper one. The reward influences the surrounding cells, and therefore, to follow the same path, the algorithm prefers the one with a higher reward.\n"],"metadata":{"id":"PD5MH95mLvCU"},"id":"PD5MH95mLvCU"},{"cell_type":"markdown","source":["## Question  3"],"metadata":{"id":"aPOMQ3KMNY8N"},"id":"aPOMQ3KMNY8N"},{"cell_type":"markdown","source":["## The algorithm relies on key data to operate within a Markov Decision Process (MDP). The state space (S) identifies all possible situations, the action space (A) specifies the agent's options, and the transition probabilities (P) indicate how actions affect states. The reward function (R) assigns values to state-action interactions, and the discount factor (Î³) weighs future rewards.\n"],"metadata":{"id":"jEkPJjuZNf8d"},"id":"jEkPJjuZNf8d"},{"cell_type":"markdown","source":["## Question 4"],"metadata":{"id":"4uxthIsrNcl0"},"id":"4uxthIsrNcl0"},{"cell_type":"markdown","source":["## In controlled environments, providing accurate information is feasible, but in dynamic contexts, obtaining exact probabilities and rewards can be challenging. In real-world MDPs, approximations and assumptions are often used to simplify the problem.\n"],"metadata":{"id":"C5EYWyaDNgk8"},"id":"C5EYWyaDNgk8"},{"cell_type":"markdown","source":["## Question 5"],"metadata":{"id":"Sxvbp5KVNd1W"},"id":"Sxvbp5KVNd1W"},{"cell_type":"markdown","source":["## Although we have focused on discrete states and actions, the MDP can be adapted to continuous spaces. Techniques such as discretization can be applied to continuous states, and there are specific algorithms for continuous MDPs, extending the applicability of the framework to more diverse domains."],"metadata":{"id":"hjBEndRxNhUc"},"id":"hjBEndRxNhUc"}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[],"collapsed_sections":["159addea","de8bb41f","394de466","3bd8d6e9","f1d30a7f","ce81e31b","l8lBWmIY9xkN","hO_EE9DX9xkO","tQYVBs8REpdV","tFVB4bbBFizy","tw7cZt3jIGBc","qdxwHikuJQhE"]}},"nbformat":4,"nbformat_minor":5}