{"cells":[{"cell_type":"markdown","metadata":{"id":"ZBYubWzg24KC"},"source":["# Experience Replay"]},{"cell_type":"markdown","metadata":{"id":"y0aDVPO22_QB"},"source":["As off-policy methods use a different policy for exploring the observation space (behavior policy) and value-function update (target policy), it is possible to use a memory buffer of experiences, i.e., (observation, action, reward) tuples, during the training process."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":22942,"status":"ok","timestamp":1701168419627,"user":{"displayName":"Iker Etxebeste Arroyo","userId":"06101170301682302069"},"user_tz":-60},"id":"NFzsC-Js3xXA","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1b8df6c9-310d-4927-a3e8-2efcb383f15d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gymnasium\n","  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.23.5)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.5.0)\n","Collecting farama-notifications>=0.0.1 (from gymnasium)\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Installing collected packages: farama-notifications, gymnasium\n","Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n","Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (5.15.0)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly) (8.2.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly) (23.2)\n","OpenAI Gym version: 0.29.1\n"]}],"source":["\n","!pip install gymnasium\n","!pip install plotly\n","\n","import gymnasium as gym\n","import numpy as np\n","from collections import deque\n","import random\n","import plotly.express as px\n","\n","print(f\"OpenAI Gym version: {gym.__version__}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lU5K5J-viAMh"},"outputs":[],"source":["# Video management imports\n","import cv2\n","\n","# Helper functions to save videos and images\n","def save_video(img_array, path='./video/test.mp4'):\n","  height, width, layers = img_array[0].shape\n","  size = (width, height)\n","  out = cv2.VideoWriter(path, cv2.VideoWriter_fourcc(*'AVC1'), 15, size)\n","  for i in range(len(img_array)):\n","    bgr_img = cv2.cvtColor(img_array[i], cv2.COLOR_RGB2BGR)\n","    out.write(bgr_img)\n","  out.release()\n","  print('Video saved.')\n","\n","def save_images(img_array, path='./images'):\n","  for i, image in enumerate(img_array):\n","    bgr_img = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","    cv2.imwrite(path + '/img_' + str(i) + '.jpg', bgr_img)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PQjRs4TwiAMh","executionInfo":{"status":"ok","timestamp":1701168453106,"user_tz":-60,"elapsed":33492,"user":{"displayName":"Iker Etxebeste Arroyo","userId":"06101170301682302069"}},"outputId":"d6c59579-f295-4a14-a79f-452da2ff2294"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running in Google Colab\n","Mounted at /content/drive\n"]}],"source":["# Check if we running in Google Colab or Jupyter Notebook\n","import sys\n","IN_COLAB = 'google.colab' in sys.modules\n","\n","if IN_COLAB:\n","    print('Running in Google Colab')\n","    # Connect with Google Drive\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    # This auxiliary function simplifies the visualization of OpenCV Images\n","    from google.colab import output\n","    def clear_ouput():\n","        ouput.clear()\n","else:\n","    from IPython.display import clear_output\n","    print('Running in Jupyter Notebook')"]},{"cell_type":"markdown","metadata":{"id":"mnOGXVYmDOTL"},"source":["## The Basic TD Agent\n","To implement the experience replay buffer, three variables to manage it were added:\n","- experience_replay_size: The size of the memory buffer.\n","- experience_replay: The memory buffer itself. It is a deque, as we want to maintain the most recent experience in the buffer.\n","- minibatch_size: Size of the sample extracted from memory to update the state value function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wvjBjVRb34NC"},"outputs":[],"source":["class TDAgent:\n","  def __init__(self, /, num_states, num_actions, gamma=1.0, alpha=0.5, epsilon = 0.1, experience_replay_size=1000, minibatch_size = 32):\n","    # Future discount parameter\n","    self.gamma = gamma\n","    # Learning rate\n","    self.alpha = alpha\n","    # Epsilon parameter for the e-greedy policy\n","    self.epsilon = epsilon\n","    # Number of states\n","    self.num_states = num_states\n","    # Number of actions\n","    self.num_actions = num_actions\n","    # Q-values\n","    self.q_values = np.zeros((self.num_states, self.num_actions))\n","    # Size of the experience replay buffer\n","    self.experience_replay_size = experience_replay_size\n","    # Experience replay buffer\n","    self.experience_replay = deque(maxlen=self.experience_replay_size)\n","    # Size of the sample to update the value function\n","    self.minibatch_size = minibatch_size\n","\n","  def choose_policy_action(self, observation):\n","    \"\"\"Chooses the action with the maximum q-value.\n","\n","    Args:\n","      observation: An observation from the environment.\n","    Returns:\n","      The greedy action (the one with best value).\n","    \"\"\"\n","    return self.argmax(self.q_values[observation, :])\n","\n","  def choose_e_greedy_action(self, observation):\n","    \"\"\"Chooses an action following the e-greedy exploration method.\n","\n","    Args:\n","      observation: An observation from the environment.\n","    Returns:\n","      The e-greedy action. With probability e, it returns a random action, which\n","      includes the policy action, otherwise, with probability 1 - e, it returns\n","      the policy action.\n","    \"\"\"\n","    if np.random.rand() < self.epsilon:\n","      return np.random.choice(range(self.num_actions))\n","    else:\n","      return self.choose_policy_action(observation)\n","\n","  def argmax(self, np_array):\n","    \"\"\"argmax method with random tie-breaking.\n","\n","    Args:\n","      np_array: A numpy array.\n","    Returns:\n","      Index of one of the appearances of the highest value in the array.\n","    \"\"\"\n","    tie_indices = np.flatnonzero(np_array == np_array.max())\n","    return np.random.choice(tie_indices)"]},{"cell_type":"markdown","metadata":{"id":"9iQ7ZV5kEDKb"},"source":["## The Q-Learning Agent\n","\n","When using the experience replay buffer, the update step changes:\n","\n","- We have to extract a sample of experience: the minibatch.\n","- For each sample, update the action value function: minibatch update.\n","\n","Remember the step for the Q-learning without batches:\n","\n","```\n","def step(self):\n","  td_target = reward\n","  if(not done):\n","    td_target += self.gamma * np.max(self.q_values[next_observation, :])\n","  self.q_values[self.observation, self.action] += self.alpha * (td_target - self.q_values[self.observation, self.action])\n","```\n","\n","We will do the same but for arrays of observations, actions, rewards, and next observations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B9J6yQ5u38wp"},"outputs":[],"source":["class QLearningAgent(TDAgent):\n","  def minibatch_step(self):\n","    # Get from the experience replay buffer a minibatch\n","    minibatch = random.sample(self.experience_replay, self.minibatch_size)\n","    # Separate in different arrays observations, actions, rewards, dones, and\n","    # next_observations.\n","    observations, actions, rewards, dones, next_observations = zip(*minibatch)\n","    # Convert them to np.arrays\n","    observations = np.array(observations)\n","    actions = np.array(actions)\n","    rewards = np.array(rewards, dtype=np.float64)\n","    dones = np.array(dones)\n","    next_observations = np.array(next_observations)\n","\n","    # Instead of a single TD target, an array of TD targets is computed\n","    # For the final state, the TD target is the last reward.\n","    td_targets = np.copy(rewards)\n","    # For the rest of the states we need to add the expected return.\n","    td_targets[dones == False] += self.gamma * np.max(self.q_values[next_observations, :], axis=1)[dones == False]\n","    # Then do the Bellman update\n","    self.q_values[observations, actions] += self.alpha * (td_targets - self.q_values[observations, actions])"]},{"cell_type":"markdown","metadata":{"id":"KZtPW4gsiAMi"},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S0Jd04314Aug"},"outputs":[],"source":["# Num of episodes to learn\n","EPISODES = 100\n","# Experience replay buffer size\n","MEMORY_SIZE = 2000\n","# Size of the batch extracted from the experience replay buffer\n","MINIBATCH_SIZE = 32\n","\n","# Initialize the environment\n","env = gym.make('CliffWalking-v0', render_mode=\"rgb_array\")\n","\n","# Define agent\n","agent = QLearningAgent(num_states=env.observation_space.n, num_actions= env.action_space.n, gamma= 1.0, alpha= 0.9, epsilon=0.1, experience_replay_size= MEMORY_SIZE, minibatch_size=MINIBATCH_SIZE)\n","# Tracking the training\n","visit_count = np.zeros(agent.num_states)\n","\n","total_steps = 0\n","for episode in range(EPISODES):\n","    # Reset episode variables and enviroment\n","    done = False\n","    observation, _ = env.reset()\n","    visit_count[observation] += 1\n","    while not done:\n","      total_steps += 1\n","      # Choose action\n","      action = agent.choose_e_greedy_action(observation)\n","      # Take a step in env\n","      next_observation, reward, terminated, truncated, _ = env.step(action)\n","      done = terminated or truncated\n","\n","      # Append experience to the experience replay buffer\n","      agent.experience_replay.append((observation, action, reward, done, next_observation))\n","      # Check if the buffer has the minimum size for extracting a minibatch\n","      if len(agent.experience_replay) > agent.minibatch_size:\n","        # Have the agent update its Q-Values\n","        agent.minibatch_step()\n","      # Update agent observation to new one\n","      observation = next_observation\n","      visit_count[observation] += 1"]},{"cell_type":"markdown","metadata":{"id":"yG1IKOJ4GCHM"},"source":["## Training Evolution"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"executionInfo":{"elapsed":1767,"status":"ok","timestamp":1701168455288,"user":{"displayName":"Iker Etxebeste Arroyo","userId":"06101170301682302069"},"user_tz":-60},"id":"zjjn-SOPDDtX","outputId":"932e89b1-5b9c-40f3-e0a4-b70184f52269"},"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"57f8420e-e5ed-4709-869b-d2b021976465\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"57f8420e-e5ed-4709-869b-d2b021976465\")) {                    Plotly.newPlot(                        \"57f8420e-e5ed-4709-869b-d2b021976465\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":[[10.0,6.0,8.0,17.0,13.0,8.0,15.0,12.0,9.0,18.0,13.0,14.0],[12.0,13.0,15.0,17.0,12.0,14.0,18.0,14.0,9.0,16.0,14.0,15.0],[166.0,156.0,148.0,149.0,136.0,128.0,129.0,129.0,117.0,116.0,112.0,116.0],[174.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,100.0]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"x: %{x}\\u003cbr\\u003ey: %{y}\\u003cbr\\u003ecolor: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\"},\"coloraxis\":{\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('57f8420e-e5ed-4709-869b-d2b021976465');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{}}],"source":["fig = px.imshow(visit_count.reshape((4, 12)))\n","fig.show()"]},{"cell_type":"markdown","metadata":{"id":"8QudmTbwF77t"},"source":["## Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1701168455288,"user":{"displayName":"Iker Etxebeste Arroyo","userId":"06101170301682302069"},"user_tz":-60},"id":"3COWwr5_4Mts","outputId":"69861e04-0eb4-4524-90af-550f9e761913"},"outputs":[{"output_type":"stream","name":"stdout","text":["Video saved.\n"]}],"source":["observation, _ = env.reset()\n","done = False\n","images = []\n","while not done:\n","    action = agent.choose_policy_action(observation)\n","    observation, reward, terminated, truncated, _ = env.step(action)\n","    done = terminated or truncated\n","    image = env.render()\n","    images.append(image)\n","\n","    if len(images) > 100:\n","        done = True\n","\n","save_video(images, path='video/Q-Learning-with-experience.mp4')"]},{"cell_type":"markdown","source":["## 1. The use of experience replay is expected to result in a different visit count compared to basic Q-learning. Experience replay helps the agent revisit and learn from past experiences, reducing the impact of temporal correlations in the training data. As a result, the agent's exploration and learning behavior can be more effective and stable.\n","\n"],"metadata":{"id":"fxTZkLTkNh3t"}}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["mnOGXVYmDOTL","9iQ7ZV5kEDKb","KZtPW4gsiAMi","yG1IKOJ4GCHM","8QudmTbwF77t"]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}